---
title: "Stats 315B Homework"
author: "Rachael Caelie (Rocky) Aikens, Daniel Sosa, Christine Tataru"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, error = FALSE)

library(ggplot2)

#$ \Sigma_{m=1}^M $


```

# Problem 1

First, some notation.  Let $K^{(l)}$ denote the number of nodes in the $l^{th}$ layer of a neural net, and let $l = 0 ... L$ index the layers of the neural net, with $l=0$ denoting the input layer and $l = L$ denoting the output layer.  Additionally, we define: 
$$\delta_j^{(l)} = \frac{\partial q}{\partial a_j^{(l)}},$$
Where $a_j^{(l)} = \sum_{j = 0}^{K^{(l)}}w_{ij}o_i^{(l-1)}$.

Here, I am assuming that we are applying stochastic gradient descent and I have dropped the subscript ($t$) denoting the observation we are using, for the purposes of this derivation. I also assume that $o_0^{(l)} = 1$ for each layer, denoting the intercept inputs to each layer

To define the update rule, we need to calculate $G(w_{ij}^{(l)}) = \frac{\partial q}{\partial w_{ij}^{(l)}}$.  Applying the chain rule, this is:

\begin{align*}
\frac{\partial q}{\partial w_{ij}^{(l)}} &= \frac{\partial q}{\partial a_{j}^{(l)}}\frac{\partial a_{j}^{(l)}}{\partial w_{ij}^{(l)}} \\
&= \delta_j^{(l)}o_{i}^{(l-1)}
\end{align*}

Moreover, we already know that $\delta^{(L)} = y - o^{(L)},$ the error for this example from forward propagation.  Applying the chain rule, we can recursively calculate $\delta_j^{(l)}$ backwards through the network as:

$$\delta_j^{(l)} = S'(a_j^{(l)}) \sum_{k = 1}^{K^{(l+1)}}\delta_k^{(l+1)}w_{jk}.$$

For the sigmoid activation function, this is:

$$\delta_j^{(l)} = o_j^{(l)}(1-o_j^{(l)}) \sum_{k = 1}^{K^{(l+1)}}\delta_k^{(l+1)}w_{jk}.$$

Now we can write an algorithm for backpropagation (let $e$ denote the error from forward propagation, $O$ the outputs from each node in forward propagation, $x$ the example we are using, and $W$ the weights for the whole network).

\pagebreak

Backprop($e, O, W, x$):

\setlength{\leftskip}{1cm}

  $\delta^{(l)} = e$
  
  $g(w_i^{(l)}) = \delta^{(l)}o_i^{(l)}$
  
  For $l = L-1$ to 1:
  
\setlength{\leftskip}{2cm}
  
$\{\delta_j^{(l)} = S'(a_j^{(l)}) \sum_{k = 1}^{K^{(l+1)}}\delta_k^{(l+1)}w_{jk} \text{ for each } j = 1...K^{(l)}\}$
    
$\{g(w_{ij}^{(l)}) = \delta_j^{(l)}o_i^{(l-1)} \text{ for each } j = 1...K^{(l)}, i = 0... K^{(l-1)}\}$

\setlength{\leftskip}{1cm}
    
$\{w_{ij}^{(l)} \leftarrow w_{ij} -\eta g(w_{ij}^{(l)})\text{ for each } j = 1...K^{(l)}, i = 0... K^{(l-1)}, l = 1...L\}$
  
\setlength{\leftskip}{0cm}

# Problem 2

\begin{align}
\frac{\partial\hat{F}}{\partial a_m} &= B(\textbf{x}|\mu_m, \sigma_m) \\
\frac{\partial\hat{F}}{\partial \sigma_m} &= \Sigma_{m=1}^M a_m (\frac{1}{ \sigma_m^3}\Sigma_{j=1}^n(x_j-\mu_{jm})^2)B(\textbf{x}|\mu_m, \sigma_m) \\ \frac{\partial\hat{F}}{\partial \mu_{mj}} &= \Sigma_{m=1}^M (\frac{a_m}{ \sigma_m^2}(x_j-\mu_{jm}))B(\textbf{x}|\mu_m, \sigma_m) 
\end{align}

# Problem 3

We can show that the ellipsoidal radial basis function can be written as the spherical gaussian basis function:

$$\exp\left\{-\frac{1}{2}\sum_{j=1}^p(\tilde{x}_j - \tilde{\mu}_j)^2\right\},$$
For some transformed $\tilde{x}$ and $\tilde{\mu}$. 

Simply notice that $\Sigma$ is a positive semidefinite matrix, so it has a positive semidefinite square root, $\Sigma^{1/2}$. Moreover, any positive semidefinite matrix is symmetric and invertible. We can then manipulate the ellipsoidal basis function as follows:

\begin{align*}
B(x|\mu_m, \Sigma) &= \exp\left\{-\frac{1}{2}(x-\mu_m)^T\Sigma(x-\mu_m)\right\}\\
&= \exp\left\{-\frac{1}{2}(x-\mu_m)^T\Sigma^{1/2}\Sigma^{1/2}(x-\mu_m)\right\}\\
&= \exp\left\{-\frac{1}{2}(\Sigma^{1/2}(x-\mu_m))^T(\Sigma^{1/2}(x-\mu_m))\right\}
\end{align*}

Letting $\Sigma^{1/2}x = \tilde{x}$ and $\Sigma^{1/2}\mu_m = \tilde{\mu},$ we retrieve the spherical basis function desired. Note also that $\Sigma^{1/2}$ is invertible, so we can easily convert back to the original $x$ and $\mu_m$ with the inverse transformation $x = \Sigma^{-1/2}\tilde{x}$ and $\mu_m = \Sigma^{-1/2}\tilde{\mu}$.

# Problem 4

For the elliptical radial basis function to vary only in one direction, $\Sigma$ must be some matrix so that $B(x|\mu_m, \Sigma)$ is proportional to some univariate normal p.d.f.. This requires that $\Sigma$ must be rank 1. If we require that $\Sigma$ is restricted to the set of symmetric matrices (this not explicit in the problem statement but is necessary to maintain the analogy to the gaussian p.d.f.), this is simply the set of all $p \times p$ matrices with a single nonzero diagonal entry and all other entries equal to zero.
